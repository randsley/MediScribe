# Device Testing Plan - With Final MLX Model

**Date**: February 3, 2026
**Status**: Ready for Device Testing with Real MLX Model
**Model Available**: âœ… YES (3 quantization levels tested)
**Python Validation**: âœ… COMPLETE

---

## Model Status: Production Ready

### Available Models
```
Models Directory: /Users/nigelrandsley/MediScribe/Models/

âœ… medgemma-4b-it-4bit    (2.8 GB) - Fastest, Recommended
âœ… medgemma-4b-it-6bit    (3.9 GB) - Balanced
âœ… medgemma-4b-it-8bit    (4.5 GB) - Highest Quality
```

### Tested on Python/MLX
- âœ… All 3 quantizations work
- âœ… Inference times: 1,055-1,473ms (1-1.5 seconds)
- âœ… Valid output for medical queries
- âœ… No gibberish or degradation
- âœ… Proper Gemma3 chat format

### Performance Comparison
| Quantization | Size | Speed | Recommendation |
|--------------|------|-------|-----------------|
| **4-bit** | 2.8 GB | 1,056 ms | âœ… **RECOMMENDED** |
| 6-bit | 3.9 GB | 1,307 ms | Optional |
| 8-bit | 4.5 GB | 1,473 ms | Only if needed |

---

## Device Testing Plan: Three Phases

### **Phase 1: Bundle & Deploy (This Week)** ðŸš€ IMMEDIATE

**Objective**: Get 4-bit model into iOS app and deploy to device

**Steps**:

1. **Choose Quantization**
   ```
   Selected: 4-bit (medgemma-4b-it-4bit)
   Reason: Best balance of speed (1 sec) and quality
   Size: 2.8 GB (reasonable for device storage)
   ```

2. **Prepare Model Bundle for iOS**
   ```
   Create: App/Assets/Models/medgemma-4b-it/
   Copy from: Models/medgemma-4b-it-4bit/
   Include:
   â”œâ”€â”€ model.safetensors
   â”œâ”€â”€ tokenizer.json
   â”œâ”€â”€ config.json
   â”œâ”€â”€ generation_config.json
   â”œâ”€â”€ preprocessor_config.json
   â””â”€â”€ processor_config.json
   ```

3. **Update MLXModelLoader Path**
   ```swift
   // In MLXModelLoader.swift, setupModelPath()
   // Point to bundled model:
   Bundle.main.path(forResource: "medgemma-4b-it", ofType: nil)
   ```

4. **Uncomment MLX Imports in MLXMedGemmaBridge.swift**
   ```swift
   #if !targetEnvironment(simulator)
   import MLX          // Uncomment
   import MLXNN        // Uncomment
   import MLXVLM       // Uncomment
   #endif
   ```

5. **Build and Deploy**
   ```bash
   xcodebuild -project MediScribe.xcodeproj \
              -scheme MediScribe \
              -configuration Release \
              -destination 'generic/platform=iOS' \
              build
   # Install on device via Xcode
   ```

**Timeline**: 2-3 hours
**Risk**: LOW (model is validated, infrastructure is ready)

---

### **Phase 2: Real-World Device Testing (This Week)** âœ… VALIDATION

**Objective**: Validate model works on actual iOS device with Metal GPU

**Setup**:
- Connect iPhone (Sarov or Seversk)
- Run deployed app with 4-bit model

**Test Cases**:

#### A. Basic Model Loading
```
â˜‘ App launches without crash
â˜‘ Model loads on first use
â˜‘ Memory usage reasonable (< 4GB)
â˜‘ No thermal throttling
```

#### B. SOAP Note Generation
```
â˜‘ Generate SOAP note with patient data
â˜‘ Model produces valid output (~1-2 seconds)
â˜‘ Output passes validation (no forbidden phrases)
â˜‘ Validation errors display correctly
â˜‘ UI remains responsive during generation
```

#### C. Error Handling (from ViewModel work)
```
â˜‘ Invalid output caught and blocked
â˜‘ Error message displays to user
â˜‘ User can edit and retry
â˜‘ No crashes on edge cases
```

#### D. Performance Metrics
```
â˜‘ Measure actual inference time on device
â˜‘ Compare to Python benchmark (should be similar)
â˜‘ Monitor memory during generation
â˜‘ Check battery impact
â˜‘ Monitor thermal behavior
```

#### E. Integration Testing
```
â˜‘ Save to Core Data works
â˜‘ Retrieve encrypted note works
â˜‘ Multi-note queries fast
â˜‘ Index-based filtering works
â˜‘ Encryption/decryption on device
```

#### F. Streaming Generation (if implemented)
```
â˜‘ Token streaming updates UI
â˜‘ Progress indicator works
â˜‘ Cancellation works
â˜‘ No memory leaks
```

**Expected Results**:
- âœ… Model inference: ~1-1.5 seconds (per Python testing)
- âœ… SOAP note generation complete end-to-end
- âœ… Validation errors work as designed
- âœ… No crashes or thermal issues

**Timeline**: 4-6 hours
**Risk**: LOW (model validated, infrastructure ready)

---

### **Phase 3: Optimization & Release (If Needed)**

**Objective**: Fine-tune for production if Phase 2 reveals issues

**Potential Optimizations**:
```
If memory pressure:
  â†’ Use 4-bit quantization (already selected)
  â†’ Implement lazy loading
  â†’ Unload after use

If slow inference:
  â†’ Measure GPU utilization
  â†’ Check for CPU fallback
  â†’ Profile on device

If thermal issues:
  â†’ Reduce batch size
  â†’ Add inference cooling breaks
  â†’ Implement throttling
```

**Timeline**: 2-4 hours (if needed)
**Risk**: MEDIUM (depends on Phase 2 findings)

---

## Device Testing Checklist

### Pre-Testing Setup
```
â˜‘ Model selected: 4-bit (2.8GB)
â˜‘ Model validated: Python tests pass
â˜‘ Model path: Models/medgemma-4b-it-4bit/
â˜‘ ViewModel: Error handling complete
â˜‘ iOS device: Sarov or Seversk ready
â˜‘ Xcode: Latest version, device registered
â˜‘ Storage: Device has 5GB+ free space
```

### Phase 1: Bundle & Deploy
```
â˜‘ Copy 4-bit model to App/Assets/Models/
â˜‘ Update MLXModelLoader.setupModelPath()
â˜‘ Uncomment MLX imports in MLXMedGemmaBridge.swift
â˜‘ Update deployment target to iOS 17+
â˜‘ Set up device provisioning
â˜‘ Build for device (not simulator)
â˜‘ Deploy to iPhone via Xcode
â˜‘ App launches without crash
```

### Phase 2: Functional Testing
```
LOADING:
â˜‘ Model loads on first use
â˜‘ Memory usage < 4GB
â˜‘ No thermal throttling

GENERATION:
â˜‘ Generate SOAP note with patient data
â˜‘ Model responds in ~1-1.5 seconds
â˜‘ Output is valid medical text
â˜‘ No gibberish or repetition

VALIDATION:
â˜‘ Valid output passes validation
â˜‘ Invalid output blocked
â˜‘ Error message clear
â˜‘ User can retry

INTEGRATION:
â˜‘ Save to Core Data works
â˜‘ Retrieve note works
â˜‘ Encryption/decryption works
â˜‘ Multiple notes work

PERFORMANCE:
â˜‘ Measure inference time
â˜‘ Monitor memory
â˜‘ Check thermal behavior
â˜‘ Verify battery impact
```

### Phase 3: Edge Cases
```
â˜‘ Network disabled (should work offline)
â˜‘ Low memory (test with 512MB free)
â˜‘ Extended use (generate multiple notes)
â˜‘ Interrupted generation (user cancels)
â˜‘ Concurrent operations (multiple notes at once)
```

---

## Model Integration: Technical Details

### Current State
```
MLXMedGemmaBridge.swift:
â”œâ”€â”€ #if targetEnvironment(simulator)
â”‚   â””â”€â”€ Placeholder implementation
â””â”€â”€ #else
    â””â”€â”€ Real MLX model (needs uncommenting)
```

### What Needs to Happen

1. **Uncomment MLX Imports**
   ```swift
   // In MLXMedGemmaBridge.swift, line ~15
   #if !targetEnvironment(simulator)
   import MLX              // UNCOMMENT
   import MLXNN            // UNCOMMENT
   import MLXVLM           // UNCOMMENT
   import MLXLMCommon      // UNCOMMENT
   #endif
   ```

2. **Bundle Model with App**
   ```
   Xcode Project Settings:
   â”œâ”€ Add Model Files to Target
   â”œâ”€ Bundle Resources: Include medgemma-4b-it/
   â””â”€ Copy Files: medgemma-4b-it/ to App Bundle
   ```

3. **Update Model Path**
   ```swift
   // In MLXModelLoader.setupModelPath()
   guard let bundlePath = Bundle.main.bundlePath else { return }
   let modelPath = "\(bundlePath)/medgemma-4b-it"
   ```

4. **Ensure Device Build**
   ```
   Build Settings:
   â”œâ”€ Architecture: ARM64 (Apple Silicon)
   â”œâ”€ Platform: iOS 17+
   â”œâ”€ Metal: Enabled (for GPU acceleration)
   â””â”€ Deployment Target: iPhone compatible
   ```

---

## Performance Expectations

### From Python Testing
```
Quantization: 4-bit
Model: medgemma-4b-it-4bit
Inference Time: ~1,056 ms (1 second)
Output Quality: Valid medical text
GPU Required: Yes (Metal on iOS)
```

### On iOS Device
```
Expected similar to Python:
- Inference: 1-2 seconds (on Metal GPU)
- Memory: 2.8GB for model + overhead
- Thermal: Manageable (brief inference)
- Battery: ~5-10% per generation
```

### Why Device Might Differ
```
Factors that might change performance:
- Device CPU/GPU generation (newer = faster)
- Ambient temperature (thermal throttling)
- Available free memory
- Other background apps
- iOS version
```

---

## Risk Assessment

### Low Risk âœ…
- Model validated on Python/MLX
- Infrastructure tested (ViewModel, validation)
- Build system ready
- Device available

### Medium Risk âš ï¸
- First time running on physical iOS device
- Metal GPU optimization unknown
- Device-specific thermal issues possible
- Model size (2.8GB) on limited storage

### Mitigation
```
âœ… Have fallback plan (4-bit is smallest)
âœ… Monitor thermal during testing
âœ… Start with short generations
âœ… Have simulator backup for quick iteration
```

---

## Success Criteria

### Phase 1 Success
```
âœ… App builds and deploys to device
âœ… Model loads without crash
âœ… Memory usage acceptable
âœ… App remains responsive
```

### Phase 2 Success
```
âœ… SOAP note generates in 1-2 seconds
âœ… Output is valid (no gibberish)
âœ… Validation works as designed
âœ… Errors are handled gracefully
âœ… Core Data operations work
âœ… Encryption/decryption works
âœ… Performance acceptable for clinical use
```

### Overall Device Testing Success
```
âœ… Model works on actual iOS device
âœ… All features function end-to-end
âœ… Error handling robust
âœ… Performance acceptable
âœ… Ready for user testing
```

---

## Timeline

### This Week (Feb 3-7)
```
Monday (2-4 hrs):
  â†’ Bundle 4-bit model
  â†’ Update code paths
  â†’ Build for device

Tuesday (2-3 hrs):
  â†’ Deploy to device
  â†’ Test basic loading
  â†’ Test generation

Wednesday (4-6 hrs):
  â†’ Full functional testing
  â†’ Performance measurement
  â†’ Error case testing

Thursday (2 hrs):
  â†’ Documentation
  â†’ Results summary
  â†’ Optimization ideas
```

**Total**: ~10-15 hours over the week

---

## Next Immediate Actions

### Priority 1 (Do First - 30 min)
```
â˜‘ Verify 4-bit model location
â˜‘ Copy model files to Xcode Assets
â˜‘ Update MLXModelLoader path
```

### Priority 2 (Do Second - 1-2 hours)
```
â˜‘ Uncomment MLX imports
â˜‘ Verify build settings for iOS device
â˜‘ Build for device (not simulator)
```

### Priority 3 (Do Third - 2-3 hours)
```
â˜‘ Deploy to iPhone
â˜‘ Test basic app launch
â˜‘ Test model loading
â˜‘ Test SOAP generation
```

---

## Conclusion

**Status**: âœ… READY FOR IMMEDIATE DEVICE TESTING

**Model**: Validated (Python/MLX testing complete)
**Infrastructure**: Ready (ViewModel error handling complete)
**Timeline**: Can start this week
**Risk**: LOW (all components tested)
**Expected Outcome**: Full end-to-end validation of MediScribe with real MLX model on iOS device

**Recommendation**: **Begin Phase 1 (Bundle & Deploy) immediately**
- No blockers
- High confidence (model validated)
- Fast turnaround (2-3 hours)
- Will unblock production readiness
